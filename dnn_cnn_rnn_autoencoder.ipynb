{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    return (y==yhat).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-f07c455d09f7>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "hidden1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n",
    "hidden2 = tf.layers.dense(hidden1, 100, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden2, 10)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9584666666666667 0.9545\n",
      "1 0.9682166666666666 0.962\n",
      "2 0.9819833333333333 0.973\n",
      "3 0.9869 0.976\n",
      "4 0.9878166666666667 0.976\n",
      "5 0.99295 0.9797\n",
      "6 0.9938166666666667 0.9794\n",
      "7 0.99535 0.9801\n",
      "8 0.9977166666666667 0.9808\n",
      "9 0.9976 0.9802\n",
      "10 0.9987833333333334 0.9807\n",
      "11 0.9992666666666666 0.981\n",
      "12 0.99965 0.9828\n",
      "13 0.9997666666666667 0.9821\n",
      "14 0.9998333333333334 0.982\n",
      "15 0.9999666666666667 0.9824\n",
      "16 0.9999666666666667 0.9824\n",
      "17 0.9999833333333333 0.9825\n",
      "18 1.0 0.9827\n",
      "19 1.0 0.9827\n",
      "20 1.0 0.9823\n",
      "21 1.0 0.9823\n",
      "22 1.0 0.9824\n",
      "23 1.0 0.9822\n",
      "24 1.0 0.9821\n",
      "25 1.0 0.9826\n",
      "26 1.0 0.9821\n",
      "27 1.0 0.9826\n",
      "28 1.0 0.9825\n",
      "29 1.0 0.9827\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN (Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_dropout(X, n_neurons, training, rate=0.0, activation=tf.nn.relu, output_activation=None):\n",
    "    for l in range(len(n_neurons)):\n",
    "        X_dropout = tf.layers.dropout(X, rate=dropout_rate, training=training)\n",
    "        X = tf.layers.dense(X_dropout, n_neurons[l], activation=(activation if l<len(n_neurons)-1 else output_activation))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "training = tf.placeholder_with_default(False, ())\n",
    "logits = mlp_dropout(X, [300, 100, 10], training, rate=0.2)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9500166666666666 0.951\n",
      "1 0.96755 0.9637\n",
      "2 0.9744833333333334 0.9716\n",
      "3 0.9786 0.974\n",
      "4 0.9816 0.9754\n",
      "5 0.9849333333333333 0.9785\n",
      "6 0.9854333333333334 0.9765\n",
      "7 0.9883 0.9806\n",
      "8 0.9888 0.9807\n",
      "9 0.9897666666666667 0.9812\n",
      "10 0.99055 0.9814\n",
      "11 0.9914666666666667 0.982\n",
      "12 0.9925833333333334 0.9836\n",
      "13 0.9931 0.9841\n",
      "14 0.9938 0.9835\n",
      "15 0.9934833333333334 0.9828\n",
      "16 0.9943166666666666 0.9816\n",
      "17 0.9948166666666667 0.984\n",
      "18 0.9948333333333333 0.9813\n",
      "19 0.9952833333333333 0.9833\n",
      "20 0.9956833333333334 0.9836\n",
      "21 0.9961333333333333 0.9844\n",
      "22 0.9964666666666666 0.9855\n",
      "23 0.9963 0.9846\n",
      "24 0.9962833333333333 0.9851\n",
      "25 0.9970166666666667 0.9844\n",
      "26 0.9970333333333333 0.9857\n",
      "27 0.9968166666666667 0.9838\n",
      "28 0.9977833333333334 0.9848\n",
      "29 0.9978166666666667 0.9853\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch, training:True})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN (L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "hidden1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n",
    "hidden2 = tf.layers.dense(hidden1, 100, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden2, 10)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "scale = 1E-3\n",
    "l2_sum = [tf.reduce_sum(tf.pow(w, 2)) for w in tf.global_variables() if 'kernel:0' in w.name]\n",
    "reg_loss = 0.5 * scale * tf.reduce_sum(l2_sum)\n",
    "base_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "loss = base_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.95385 0.9516\n",
      "1 0.9642 0.9607\n",
      "2 0.9738833333333333 0.9688\n",
      "3 0.9787166666666667 0.9708\n",
      "4 0.9796833333333334 0.9717\n",
      "5 0.9859833333333333 0.9777\n",
      "6 0.98655 0.9755\n",
      "7 0.9891833333333333 0.978\n",
      "8 0.99055 0.9792\n",
      "9 0.9893666666666666 0.9785\n",
      "10 0.98855 0.9779\n",
      "11 0.9915333333333334 0.9796\n",
      "12 0.9913833333333333 0.98\n",
      "13 0.9919166666666667 0.981\n",
      "14 0.9927166666666667 0.979\n",
      "15 0.9925833333333334 0.9802\n",
      "16 0.9920333333333333 0.9791\n",
      "17 0.9917833333333334 0.9795\n",
      "18 0.9917666666666667 0.9796\n",
      "19 0.9920333333333333 0.9809\n",
      "20 0.99325 0.9817\n",
      "21 0.9926833333333334 0.9792\n",
      "22 0.9935333333333334 0.9803\n",
      "23 0.9931166666666666 0.9796\n",
      "24 0.9939166666666667 0.9795\n",
      "25 0.9935 0.98\n",
      "26 0.9853833333333334 0.9732\n",
      "27 0.99195 0.9789\n",
      "28 0.99395 0.981\n",
      "29 0.9948 0.9831\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN (Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_bn(X, n_neurons, training, momentum=0.99, activation=tf.nn.relu, output_activation=None):\n",
    "    for l in range(len(n_neurons)):\n",
    "        X_dense = tf.layers.dense(X, n_neurons[l])\n",
    "        X_bn = tf.layers.batch_normalization(X_dense, training=training, momentum=momentum)\n",
    "        X = activation(X_bn) if l < len(n_neurons)-1 else (X_bn if output_activation==None else output_activation(X_bn))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "training = tf.placeholder_with_default(False, ())\n",
    "logits = mlp_bn(X, [300,100,10], training)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1E-3\n",
    "batch_size = 100\n",
    "n_bathces = int(len(X_train) / batch_size)\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = [optimizer.minimize(loss), tf.get_collection(tf.GraphKeys.UPDATE_OPS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_bathces):\n",
    "            X_batch, y_batch = X_train[batch*batch_size : (batch+1)*batch_size], y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch, training:True})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN (Polyak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "hidden1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n",
    "hidden2 = tf.layers.dense(hidden1, 100, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden2, 10)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1E-3\n",
    "batch_size = 100\n",
    "n_bathces = int(len(X_train) / batch_size)\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [param for param in tf.global_variables() if ('kernel:0' in param.name) | ('bias:0' in param.name)]\n",
    "cache_params = [tf.Variable(tf.zeros(param.shape.dims)) for param in params]\n",
    "pol_params = [tf.Variable(tf.zeros(param.shape.dims)) for param in params]\n",
    "\n",
    "params_update_op = [tf.assign(pol_param, 0.999*pol_param+0.001*param) for pol_param, param in zip(pol_params, params)]\n",
    "assign_cache_op = [tf.assign(cache_param, param) for cache_param, param in zip(cache_params, params)]\n",
    "assign_polyak_op = [tf.assign(param, pol_param) for pol_param, param in zip(pol_params, params)]\n",
    "assign_origin_op = [tf.assign(param, cache_param) for cache_param, param in zip(cache_params, params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9624166666666667 0.9583\n",
      "0 0.95775 0.9549\n",
      "\n",
      "1 0.9755333333333334 0.9678\n",
      "1 0.9732333333333333 0.9666\n",
      "\n",
      "2 0.9813 0.9753\n",
      "2 0.9803 0.9734\n",
      "\n",
      "3 0.97625 0.966\n",
      "3 0.9853666666666666 0.9766\n",
      "\n",
      "4 0.9901166666666666 0.976\n",
      "4 0.9896666666666667 0.9795\n",
      "\n",
      "5 0.9917666666666667 0.9802\n",
      "5 0.9931333333333333 0.9807\n",
      "\n",
      "6 0.99025 0.976\n",
      "6 0.9954666666666667 0.9823\n",
      "\n",
      "7 0.9929833333333333 0.9782\n",
      "7 0.9975 0.984\n",
      "\n",
      "8 0.99435 0.98\n",
      "8 0.9983166666666666 0.9845\n",
      "\n",
      "9 0.99555 0.9796\n",
      "9 0.9990833333333333 0.985\n",
      "\n",
      "10 0.9949166666666667 0.9787\n",
      "10 0.9993666666666666 0.9847\n",
      "\n",
      "11 0.9972 0.9817\n",
      "11 0.9995 0.9854\n",
      "\n",
      "12 0.9952333333333333 0.9782\n",
      "12 0.9996833333333334 0.9854\n",
      "\n",
      "13 0.9977 0.9824\n",
      "13 0.9998166666666667 0.9851\n",
      "\n",
      "14 0.9941333333333333 0.9769\n",
      "14 0.9998 0.9848\n",
      "\n",
      "15 0.9956666666666667 0.9781\n",
      "15 0.9999 0.9853\n",
      "\n",
      "16 0.9963333333333333 0.9809\n",
      "16 0.9999333333333333 0.9853\n",
      "\n",
      "17 0.99535 0.9787\n",
      "17 0.9999333333333333 0.9855\n",
      "\n",
      "18 0.9933666666666666 0.976\n",
      "18 0.9999166666666667 0.9856\n",
      "\n",
      "19 0.9944166666666666 0.9785\n",
      "19 0.99995 0.9855\n",
      "\n",
      "20 0.9963666666666666 0.9796\n",
      "20 0.9999333333333333 0.9852\n",
      "\n",
      "21 0.9991166666666667 0.9829\n",
      "21 1.0 0.9848\n",
      "\n",
      "22 0.9968666666666667 0.9797\n",
      "22 1.0 0.9842\n",
      "\n",
      "23 0.9990833333333333 0.9808\n",
      "23 0.9999833333333333 0.9845\n",
      "\n",
      "24 0.9978166666666667 0.9805\n",
      "24 0.9999833333333333 0.9838\n",
      "\n",
      "25 0.9987 0.9807\n",
      "25 1.0 0.9844\n",
      "\n",
      "26 0.9979833333333333 0.9809\n",
      "26 1.0 0.9845\n",
      "\n",
      "27 0.999 0.9804\n",
      "27 1.0 0.9846\n",
      "\n",
      "28 0.9983833333333333 0.981\n",
      "28 1.0 0.9848\n",
      "\n",
      "29 0.9982 0.9812\n",
      "29 1.0 0.9849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_bathces):\n",
    "            X_batch, y_batch = X_train[batch*batch_size : (batch+1)*batch_size], y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run([training_op, params_update_op], {X:X_batch, y:y_batch})\n",
    "        yhat_train,  yhat_test = sess.run(yhat, {X:X_train}), sess.run(yhat, {X:X_test})\n",
    "        acc_train, acc_test = accuracy(y_train, yhat_train), accuracy(y_test, yhat_test)\n",
    "        sess.run([assign_cache_op, assign_polyak_op])\n",
    "        yhat_train, yhat_test = sess.run(yhat, {X:X_train}), sess.run(yhat, {X:X_test})\n",
    "        acc_train_pol, acc_test_pol = accuracy(y_train, yhat_train), accuracy(y_test, yhat_test)\n",
    "        sess.run([assign_origin_op])\n",
    "        print(epoch, acc_train, acc_test)\n",
    "        print(epoch, acc_train_pol, acc_test_pol)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1, 28, 28, 1])\n",
    "X_test = X_test.reshape([-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "C1 = tf.layers.conv2d(X, kernel_size=[5,5], filters=6, padding='SAME', activation=tf.nn.relu)\n",
    "S2 = tf.layers.max_pooling2d(C1, pool_size=[2,2], strides=2)\n",
    "C3 = tf.layers.conv2d(S2, kernel_size=[5,5], filters=16, activation=tf.nn.relu)\n",
    "S4 = tf.layers.max_pooling2d(C3, pool_size=[2,2], strides=2)\n",
    "C5 = tf.layers.conv2d(S4, kernel_size=[5,5], filters=120, activation=tf.nn.relu)\n",
    "F6 = tf.layers.dense(C5, 84, activation=tf.nn.relu)\n",
    "logits = tf.reshape(tf.layers.dense(F6, 10), [-1, 10])\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9781833333333333 0.9792\n",
      "1 0.9880333333333333 0.9865\n",
      "2 0.9880166666666667 0.9875\n",
      "3 0.9917333333333334 0.9861\n",
      "4 0.99295 0.9876\n",
      "5 0.9927166666666667 0.9887\n",
      "6 0.99575 0.9894\n",
      "7 0.9951333333333333 0.9893\n",
      "8 0.99535 0.9886\n",
      "9 0.9965166666666667 0.988\n",
      "10 0.9981333333333333 0.9909\n",
      "11 0.9968 0.9906\n",
      "12 0.9956833333333334 0.9874\n",
      "13 0.9990333333333333 0.9921\n",
      "14 0.9974666666666666 0.9909\n",
      "15 0.99765 0.9899\n",
      "16 0.9981833333333333 0.9908\n",
      "17 0.9993666666666666 0.9918\n",
      "18 0.9988666666666667 0.9914\n",
      "19 0.9996 0.9917\n",
      "20 0.9992 0.9919\n",
      "21 0.9996833333333334 0.9915\n",
      "22 0.9992833333333333 0.9904\n",
      "23 0.9975166666666667 0.9884\n",
      "24 0.9987 0.9907\n",
      "25 0.9995333333333334 0.9914\n",
      "26 0.9995166666666667 0.9919\n",
      "27 0.9999333333333333 0.9926\n",
      "28 0.9999666666666667 0.9929\n",
      "29 1.0 0.9927\n",
      "30 1.0 0.9927\n",
      "31 1.0 0.9928\n",
      "32 1.0 0.993\n",
      "33 1.0 0.9929\n",
      "34 1.0 0.993\n",
      "35 1.0 0.993\n",
      "36 1.0 0.993\n",
      "37 1.0 0.9929\n",
      "38 1.0 0.993\n",
      "39 1.0 0.9929\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1, 28, 28])\n",
    "X_test = X_test.reshape([-1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "#rnn_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(num_units=300) for _ in range(1)])\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=300)\n",
    "rnn_outputs, _ = tf.nn.dynamic_rnn(inputs=X, cell=rnn_cell, dtype=tf.float32)\n",
    "logits = tf.layers.dense(rnn_outputs[:,-1,:], 10)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.train.AdamOptimizer()\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.005, momentum=0.9)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9311833333333334 0.9341\n",
      "1 0.9595166666666667 0.9548\n",
      "2 0.9684666666666667 0.9635\n",
      "3 0.9670166666666666 0.9633\n",
      "4 0.9769166666666667 0.971\n",
      "5 0.9768166666666667 0.97\n",
      "6 0.9809 0.975\n",
      "7 0.98615 0.9778\n",
      "8 0.9810333333333333 0.9759\n",
      "9 0.9797166666666667 0.9743\n",
      "10 0.9864166666666667 0.9758\n",
      "11 0.9867166666666667 0.9785\n",
      "12 0.98895 0.9796\n",
      "13 0.9884833333333334 0.9789\n",
      "14 0.9852 0.9763\n",
      "15 0.9885833333333334 0.9753\n",
      "16 0.9911333333333333 0.9814\n",
      "17 0.9916333333333334 0.9814\n",
      "18 0.9917166666666667 0.9821\n",
      "19 0.9798833333333333 0.9719\n",
      "20 0.99325 0.9788\n",
      "21 0.9907666666666667 0.9786\n",
      "22 0.9861666666666666 0.9757\n",
      "23 0.99165 0.979\n",
      "24 0.9934666666666667 0.9803\n",
      "25 0.9918833333333333 0.9797\n",
      "26 0.99705 0.9849\n",
      "27 0.99485 0.9837\n",
      "28 0.99635 0.9813\n",
      "29 0.9919333333333333 0.9802\n",
      "30 0.9924833333333334 0.9819\n",
      "31 0.9950833333333333 0.9817\n",
      "32 0.99455 0.9834\n",
      "33 0.9965 0.9829\n",
      "34 0.9865833333333334 0.9744\n",
      "35 0.9913666666666666 0.9794\n",
      "36 0.9927 0.9802\n",
      "37 0.99735 0.981\n",
      "38 0.9941333333333333 0.9832\n",
      "39 0.9938333333333333 0.9823\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape([-1, 28, 28])\n",
    "X_test = X_test.reshape([-1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "lstm_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicLSTMCell(num_units=300) for _ in range(3)])\n",
    "lstm_outputs, _ = tf.nn.dynamic_rnn(inputs=X, cell=lstm_cell, dtype=tf.float32)\n",
    "logits = tf.layers.dense(lstm_outputs[:,-1,:], 10)\n",
    "yhat = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9744833333333334 0.9749\n",
      "1 0.9808833333333333 0.9756\n",
      "2 0.9887333333333334 0.9874\n",
      "3 0.9889333333333333 0.9874\n",
      "4 0.9918833333333333 0.9889\n",
      "5 0.9918333333333333 0.9897\n",
      "6 0.9931666666666666 0.991\n",
      "7 0.9949166666666667 0.9903\n",
      "8 0.99405 0.9884\n",
      "9 0.9948666666666667 0.9894\n",
      "10 0.9946666666666667 0.9906\n",
      "11 0.9958333333333333 0.9894\n",
      "12 0.9968 0.992\n",
      "13 0.9947 0.9887\n",
      "14 0.9962666666666666 0.99\n",
      "15 0.9973833333333333 0.9898\n",
      "16 0.99645 0.9927\n",
      "17 0.9962666666666666 0.9917\n",
      "18 0.9979 0.9923\n",
      "19 0.99725 0.9896\n",
      "20 0.99805 0.9917\n",
      "21 0.9984 0.9932\n",
      "22 0.9986666666666667 0.9931\n",
      "23 0.9986166666666667 0.9913\n",
      "24 0.99725 0.9902\n",
      "25 0.9989333333333333 0.9922\n",
      "26 0.9959 0.9907\n",
      "27 0.99865 0.9917\n",
      "28 0.9983666666666666 0.9913\n",
      "29 0.9984666666666666 0.9918\n",
      "30 0.9978333333333333 0.9901\n",
      "31 0.9983666666666666 0.9919\n",
      "32 0.9990666666666667 0.9934\n",
      "33 0.9987333333333334 0.9923\n",
      "34 0.9990833333333333 0.9926\n",
      "35 0.9991666666666666 0.9929\n",
      "36 0.9978 0.9918\n",
      "37 0.9992666666666666 0.9937\n",
      "38 0.9987 0.992\n",
      "39 0.9990666666666667 0.9914\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch, y:y_batch})\n",
    "        logits_train = sess.run(logits, {X:X_train})\n",
    "        yhat_train = sess.run(yhat, {X:X_train})\n",
    "        yhat_test = sess.run(yhat, {X:X_test})\n",
    "        print(epoch, accuracy(y_train, yhat_train), accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_encoder(X, n_neurons, activation=tf.nn.relu, encoder_activation=None, output_activation=None):\n",
    "    for i in range(len(n_neurons)):\n",
    "        if i == len(n_neurons) // 2 - 1: X = tf.layers.dense(X, n_neurons[i], activation=encoder_activation)\n",
    "        elif i == len(n_neurons) - 1: X = tf.layers.dense(X, n_neurons[i], activation=output_activation)\n",
    "        else: X = tf.layers.dense(X, n_neurons[i], activation=activation)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-00b550757562>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:514: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "Xprime = mlp_encoder(X, [1000, 500, 250, 30, 250, 500, 1000, 28*28], output_activation=tf.nn.sigmoid)\n",
    "loss = tf.losses.log_loss(labels=X, predictions=Xprime)\n",
    "mse = tf.reduce_mean(tf.reduce_sum(tf.square(X-Xprime), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "n_batches = n_train // batch_size + bool(n_train % batch_size)\n",
    "n_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20.07217 19.846277\n",
      "1 13.080404 12.902016\n",
      "2 10.2545805 10.193475\n",
      "3 8.718241 8.739309\n",
      "4 7.7372847 7.855645\n",
      "5 6.8676724 7.0577517\n",
      "6 6.2416186 6.4690638\n",
      "7 6.0546355 6.2870126\n",
      "8 5.602324 5.885851\n",
      "9 5.370997 5.695169\n",
      "10 5.22408 5.5844564\n",
      "11 5.00054 5.3623633\n",
      "12 4.8410463 5.2473145\n",
      "13 4.6245346 5.084189\n",
      "14 4.446176 4.9216037\n",
      "15 4.3591657 4.8550835\n",
      "16 4.184349 4.7242165\n",
      "17 4.1613975 4.7043295\n",
      "18 4.2076035 4.7748194\n",
      "19 3.9965494 4.5960464\n",
      "20 3.8906047 4.4964\n",
      "21 3.8281355 4.472819\n",
      "22 3.664503 4.3293343\n",
      "23 3.6297214 4.313102\n",
      "24 3.6219845 4.3172398\n",
      "25 3.9576273 4.63473\n",
      "26 3.5268195 4.247198\n",
      "27 3.4828596 4.2176876\n",
      "28 3.5239084 4.2703557\n",
      "29 3.3245168 4.0841084\n",
      "30 3.2587354 4.041132\n",
      "31 3.3717606 4.1503873\n",
      "32 3.4732218 4.244524\n",
      "33 3.276084 4.0742364\n",
      "34 3.2300558 4.0523634\n",
      "35 3.113078 3.9477625\n",
      "36 3.1138964 3.9615939\n",
      "37 3.0708404 3.9237337\n",
      "38 3.2329943 4.0761957\n",
      "39 3.0825796 3.9554892\n",
      "40 2.9855735 3.864628\n",
      "41 3.0011406 3.898843\n",
      "42 3.003555 3.9101617\n",
      "43 2.9002304 3.8263953\n",
      "44 2.902273 3.830543\n",
      "45 2.9195855 3.869682\n",
      "46 2.9371257 3.873211\n",
      "47 2.8656504 3.822407\n",
      "48 2.9061253 3.862653\n",
      "49 2.7682855 3.7423508\n",
      "50 2.8135338 3.8136034\n",
      "51 2.8108904 3.819225\n",
      "52 3.0352712 4.036029\n",
      "53 3.1837711 4.156301\n",
      "54 2.972249 3.9833899\n",
      "55 2.9054403 3.931047\n",
      "56 2.6477036 3.6997828\n",
      "57 2.7854903 3.8256054\n",
      "58 2.7148218 3.758791\n",
      "59 2.674501 3.7454515\n",
      "60 2.8764393 3.9438097\n",
      "61 2.661241 3.7417312\n",
      "62 2.6139174 3.7147684\n",
      "63 2.676725 3.7925882\n",
      "64 2.5660841 3.6839297\n",
      "65 2.6162732 3.7480109\n",
      "66 2.6461308 3.766082\n",
      "67 2.5546784 3.6940765\n",
      "68 2.593892 3.7363036\n",
      "69 2.6693008 3.810503\n",
      "70 2.5600631 3.7224793\n",
      "71 2.4949496 3.6653671\n",
      "72 2.5609994 3.726729\n",
      "73 2.5851083 3.749672\n",
      "74 2.6632318 3.8243937\n",
      "75 2.6393504 3.8002484\n",
      "76 2.465889 3.669539\n",
      "77 2.439702 3.6594157\n",
      "78 2.4849198 3.687904\n",
      "79 2.6581337 3.8511093\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in np.random.permutation(n_batches):\n",
    "            X_batch = X_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            sess.run(training_op, {X:X_batch})\n",
    "        mse_train = sess.run(mse, {X:X_train})\n",
    "        mse_test = sess.run(mse, {X:X_test})\n",
    "        print(epoch, mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_test = lgb.Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds.\n",
      "[10]\ttraining's multi_error: 0.0741\tvalid_1's multi_error: 0.0839\n",
      "[20]\ttraining's multi_error: 0.06145\tvalid_1's multi_error: 0.072\n",
      "[30]\ttraining's multi_error: 0.0549333\tvalid_1's multi_error: 0.0655\n",
      "[40]\ttraining's multi_error: 0.0487167\tvalid_1's multi_error: 0.0593\n",
      "[50]\ttraining's multi_error: 0.0436833\tvalid_1's multi_error: 0.0544\n",
      "[60]\ttraining's multi_error: 0.0388667\tvalid_1's multi_error: 0.051\n",
      "[70]\ttraining's multi_error: 0.0343333\tvalid_1's multi_error: 0.0483\n",
      "[80]\ttraining's multi_error: 0.03005\tvalid_1's multi_error: 0.0452\n",
      "[90]\ttraining's multi_error: 0.02635\tvalid_1's multi_error: 0.0436\n",
      "[100]\ttraining's multi_error: 0.0229333\tvalid_1's multi_error: 0.041\n",
      "[110]\ttraining's multi_error: 0.0194\tvalid_1's multi_error: 0.0391\n",
      "[120]\ttraining's multi_error: 0.0168167\tvalid_1's multi_error: 0.0367\n",
      "[130]\ttraining's multi_error: 0.0139833\tvalid_1's multi_error: 0.0349\n",
      "[140]\ttraining's multi_error: 0.0118\tvalid_1's multi_error: 0.0328\n",
      "[150]\ttraining's multi_error: 0.00953333\tvalid_1's multi_error: 0.0314\n",
      "[160]\ttraining's multi_error: 0.00781667\tvalid_1's multi_error: 0.0293\n",
      "[170]\ttraining's multi_error: 0.00611667\tvalid_1's multi_error: 0.0283\n",
      "[180]\ttraining's multi_error: 0.00448333\tvalid_1's multi_error: 0.0274\n",
      "[190]\ttraining's multi_error: 0.00321667\tvalid_1's multi_error: 0.0264\n",
      "[200]\ttraining's multi_error: 0.00218333\tvalid_1's multi_error: 0.0258\n",
      "[210]\ttraining's multi_error: 0.00148333\tvalid_1's multi_error: 0.0253\n",
      "[220]\ttraining's multi_error: 0.000933333\tvalid_1's multi_error: 0.0248\n",
      "[230]\ttraining's multi_error: 0.000516667\tvalid_1's multi_error: 0.0246\n",
      "[240]\ttraining's multi_error: 0.000366667\tvalid_1's multi_error: 0.0241\n",
      "[250]\ttraining's multi_error: 0.00025\tvalid_1's multi_error: 0.0239\n",
      "[260]\ttraining's multi_error: 8.33333e-05\tvalid_1's multi_error: 0.0236\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's multi_error: 0.000133333\tvalid_1's multi_error: 0.0235\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'softmax',\n",
    "    'metric': 'multi_error',\n",
    "    'num_class': 10,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'num_iterations': 500\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "               lgb_train,\n",
    "               valid_sets=[lgb_train, lgb_test],\n",
    "               early_stopping_rounds=10,\n",
    "               verbose_eval=10,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
